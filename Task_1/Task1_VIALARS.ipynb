{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\.conda\\envs\\PyTorch\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lxml import etree\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "from gensim.models.fasttext import FastText\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract data from the XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_XML_sentiment(xml, l):\n",
    "    tree = etree.parse(xml)\n",
    "    tweets = []\n",
    "    sentiments = []\n",
    "    sens = 0\n",
    "\n",
    "    for t in tree.xpath(\"database/table/column\"):\n",
    "        attribut = t.get(\"name\")        \n",
    "        if (attribut == \"text\"):\n",
    "            tweet = t.text\n",
    "            tweet = tweet.replace('&amp;', '').replace('amp;', '').replace('nbsp;', '').replace('&quot;', '').replace('&gt;', '').replace('raquo;', '').replace('mdash;', '').replace('laquo;', '').replace('RT ', '').replace('&lt;', '')\n",
    "        \n",
    "        if attribut in l:\n",
    "            if (t.text != \"NULL\"):\n",
    "                sens += int(t.text)\n",
    "\n",
    "        if (attribut == l[-1]):\n",
    "            sens = np.sign(sens)\n",
    "            if (sens != 0):\n",
    "                tweets.append(tweet)\n",
    "                sentiments.append(0 if sens == -1 else 1)\n",
    "            sens = 0\n",
    "    \n",
    "    return [tweets, sentiments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, loss, optimizer, num_epochs):      \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        \n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            x_gpu = x.to(device, dtype=torch.float)\n",
    "            y_gpu = y.to(device, dtype=torch.long)\n",
    "            prediction = model(x_gpu)\n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "def predict(model, test_loader):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            x_gpu = x.to(device, dtype=torch.float)\n",
    "\n",
    "            prediction = model(x_gpu)\n",
    "            _, indices = torch.max(prediction.data, 1)\n",
    "\n",
    "            predictions.append(indices.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from XML\n",
    "#### For bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks_list = [\"sberbank\", \"vtb\", \"gazprom\", \"alfabank\", \"bankmoskvy\", \"raiffeisen\", \"uralsib\", \"rshb\"]\n",
    "\n",
    "bank_data_train = extract_XML_sentiment(\"bank_train_2016.xml\", banks_list)\n",
    "bank_data_test = extract_XML_sentiment(\"banks_test_etalon.xml\", banks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For telecom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_list = [\"beeline\", \"mts\", \"megafon\", \"tele2\", \"rostelecom\", \"komstar\", \"skylink\"]\n",
    "\n",
    "telecom_data_train = extract_XML_sentiment(\"tkk_train_2016.xml\", telecom_list)\n",
    "telecom_data_test = extract_XML_sentiment(\"tkk_test_etalon.xml\", telecom_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Character-level convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = np.array([\"а\", \"б\", \"в\", \"г\", \"д\", \"е\", \"ё\", \"ж\", \"з\", \"и\", \"й\", \"к\", \"л\", \"м\", \"н\", \"о\",\n",
    "                     \"р\", \"с\", \"т\", \"у\", \"ф\", \"х\", \"ц\", \"ч\", \"ш\", \"щ\", \"ъ\", \"ы\", \"ь\", \"э\", \"ю\", \"я\",\n",
    "                     \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"-\", \",\", \";\", \".\", \"!\", \"?\",\n",
    "                     \":\", \"’\", \"\\\"\", \"«\", \"»\", \"/\", \"\\\\\", \"|\", \"_\", \"@\", \"#\", \"$\", \"%\", \"ˆ\", \"&\", \"*\", \n",
    "                     \"˜\", \"‘\", \"+\", \"-\", \"=\", \"<\", \">\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \" \"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_quantization(list_text):\n",
    "    list_quant = []\n",
    "    for sentence in list_text:\n",
    "        sentence_quant = []\n",
    "        for letter in sentence:\n",
    "            letter_quant = (alphabet == letter.lower())\n",
    "            letter_quant = letter_quant * 1\n",
    "            sentence_quant.append(letter_quant)\n",
    "        list_quant.append(np.array(sentence_quant).T)\n",
    "    return list_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Preparing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charCNN_Dataset(Dataset):\n",
    "    def __init__ (self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tweet = self.data[0][index]\n",
    "        miss_data = 140 - len(tweet.T)\n",
    "        data_to_add = np.zeros((len(alphabet), miss_data))\n",
    "        tweet = np.concatenate((tweet, data_to_add), axis = 1)\n",
    "        \n",
    "        sentiment = self.data[1][index]\n",
    "        \n",
    "        return tweet, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCNN_bank_data_train = deepcopy(bank_data_train)\n",
    "charCNN_bank_data_test = deepcopy(bank_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCNN_bank_data_train[0] = list_quantization(charCNN_bank_data_train[0])\n",
    "charCNN_bank_data_test[0] = list_quantization(charCNN_bank_data_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating bank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCNN_bank_dataset_train = charCNN_Dataset(charCNN_bank_data_train)\n",
    "charCNN_bank_dataset_test = charCNN_Dataset(charCNN_bank_data_test)\n",
    "\n",
    "charCNN_bank_train_loader = torch.utils.data.DataLoader(charCNN_bank_dataset_train, batch_size = batch_size)\n",
    "charCNN_bank_test_loader = torch.utils.data.DataLoader(charCNN_bank_dataset_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load telecom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCNN_telecom_data_train = deepcopy(telecom_data_train)\n",
    "charCNN_telecom_data_test = deepcopy(telecom_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCNN_telecom_data_train[0] = list_quantization(charCNN_telecom_data_train[0])\n",
    "charCNN_telecom_data_test[0] = list_quantization(charCNN_telecom_data_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating telecom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCNN_telecom_dataset_train = charCNN_Dataset(charCNN_telecom_data_train)\n",
    "charCNN_telecom_dataset_test = charCNN_Dataset(charCNN_telecom_data_test)\n",
    "\n",
    "charCNN_telecom_train_loader = torch.utils.data.DataLoader(charCNN_telecom_dataset_train, batch_size = batch_size)\n",
    "charCNN_telecom_test_loader = torch.utils.data.DataLoader(charCNN_telecom_dataset_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(78, 256, kernel_size = 7, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 3))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size = 7, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 3))\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 3))\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        \n",
    "        self.fc3 = nn.Linear(1024, 2)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                m.weight.data.normal_(0, 0.05)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.05)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)        \n",
    "        x = x.view(x.size(0), -1)        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For bank data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCNN_bank = CharCNN()\n",
    "\n",
    "charCNN_bank.type(torch.cuda.FloatTensor)\n",
    "charCNN_bank.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = optim.Adam(charCNN_bank.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-measure with macro averaging =  0.5766053616967375\n",
      "F1-measure with micro averaging =  0.7122774133083412\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_model(charCNN_bank, charCNN_bank_train_loader, loss, optimizer, 100)\n",
    "        \n",
    "charCNN_bank_predictions = predict(charCNN_bank, charCNN_bank_test_loader)\n",
    "\n",
    "f1_macro_score = f1_score(charCNN_bank_data_test[1], charCNN_bank_predictions, average = 'macro')\n",
    "f1_micro_score = f1_score(charCNN_bank_data_test[1], charCNN_bank_predictions, average = 'micro')\n",
    "\n",
    "print(\"F1-measure with macro averaging = \", f1_macro_score)\n",
    "print(\"F1-measure with micro averaging = \", f1_micro_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For telecom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCNN_telecom = CharCNN()\n",
    "\n",
    "charCNN_telecom.type(torch.cuda.FloatTensor)\n",
    "charCNN_telecom.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = optim.Adam(charCNN_telecom.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-measure with macro averaging =  0.621301057533715\n",
      "F1-measure with micro averaging =  0.7402164862614488\n",
      "Wall time: 6min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_model(charCNN_telecom, charCNN_telecom_train_loader, loss, optimizer, 100)\n",
    "        \n",
    "charCNN_telecom_predictions = predict(charCNN_telecom, charCNN_telecom_test_loader)\n",
    "\n",
    "f1_macro_score = f1_score(charCNN_telecom_data_test[1], charCNN_telecom_predictions, average = 'macro')\n",
    "f1_micro_score = f1_score(charCNN_telecom_data_test[1], charCNN_telecom_predictions, average = 'micro')\n",
    "\n",
    "print(\"F1-measure with macro averaging = \", f1_macro_score)\n",
    "print(\"F1-measure with micro averaging = \", f1_micro_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Convolutional Neural Networks for Sentence Classification\n",
    "### Using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load fasttext model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = FastText.load_fasttext_format('cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word to vector function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_vectors(model, list_text):\n",
    "    list_vect = []\n",
    "    for sentence in list_text:\n",
    "        sentence_vect = []\n",
    "        for word in sentence.split():\n",
    "            if word.lower() in model.wv.vocab:\n",
    "                sentence_vect.append(model.wv[word])\n",
    "            else:\n",
    "                sentence_vect.append(np.zeros(model.vector_size))\n",
    "        list_vect.append(np.array(sentence_vect).T)\n",
    "    return list_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SC_Dataset(Dataset):\n",
    "    def __init__ (self, data, model_vect_size):\n",
    "        self.data = data\n",
    "        self.model_vect_size = model_vect_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tweet = self.data[0][index]\n",
    "        miss_data = 30 - len(tweet.T)\n",
    "        data_to_add = np.zeros((self.model_vect_size, miss_data))        \n",
    "        tweet = np.concatenate([tweet, data_to_add], axis = 1)\n",
    "        \n",
    "        sentiment = self.data[1][index]\n",
    "        \n",
    "        return tweet, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_bank_data_train = deepcopy(bank_data_train)\n",
    "sc_bank_data_test = deepcopy(bank_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc_bank_data_train[0] = convert_to_vectors(fasttext_model, sc_bank_data_train[0])\n",
    "sc_bank_data_test[0] = convert_to_vectors(fasttext_model, sc_bank_data_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating bank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_bank_dataset_train = SC_Dataset(sc_bank_data_train, fasttext_model.vector_size)\n",
    "sc_bank_dataset_test = SC_Dataset(sc_bank_data_test, fasttext_model.vector_size)\n",
    "\n",
    "sc_bank_train_loader = torch.utils.data.DataLoader(sc_bank_dataset_train, batch_size = batch_size)\n",
    "sc_bank_test_loader = torch.utils.data.DataLoader(sc_bank_dataset_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load telecom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_telecom_data_train = deepcopy(telecom_data_train)\n",
    "sc_telecom_data_test = deepcopy(telecom_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc_telecom_data_train[0] = convert_to_vectors(fasttext_model, sc_telecom_data_train[0])\n",
    "sc_telecom_data_test[0] = convert_to_vectors(fasttext_model, sc_telecom_data_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating telecom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_telecom_dataset_train = SC_Dataset(sc_telecom_data_train, fasttext_model.vector_size)\n",
    "sc_telecom_dataset_test = SC_Dataset(sc_telecom_data_test, fasttext_model.vector_size)\n",
    "\n",
    "sc_telecom_train_loader = torch.utils.data.DataLoader(sc_telecom_dataset_train, batch_size = batch_size)\n",
    "sc_telecom_test_loader = torch.utils.data.DataLoader(sc_telecom_dataset_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Classification Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentClassCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentClassCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(300, 100, kernel_size = 3),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(300, 100, kernel_size = 4),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(300, 100, kernel_size = 5),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.pool1 = nn.MaxPool1d(28)\n",
    "        self.pool2 = nn.MaxPool1d(27)\n",
    "        self.pool3 = nn.MaxPool1d(26)\n",
    "        \n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(300, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)\n",
    "        c2 = self.conv2(x)\n",
    "        c3 = self.conv3(x)\n",
    "        \n",
    "        p1 = self.pool1(c1).squeeze(2)\n",
    "        p2 = self.pool1(c2).squeeze(2)\n",
    "        p3 = self.pool1(c3).squeeze(2)\n",
    "        \n",
    "        cat = torch.cat([p1, p2, p3], dim = 1)\n",
    "        d = self.drop(cat)\n",
    "    \n",
    "        fc = self.fc1(d)\n",
    "        \n",
    "        return fc        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scCNN_bank = SentClassCNN()\n",
    "\n",
    "scCNN_bank.type(torch.cuda.FloatTensor)\n",
    "scCNN_bank.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = optim.Adam(scCNN_bank.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-measure with macro averaging =  0.7825898509501852\n",
      "F1-measure with micro averaging =  0.837863167760075\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_model(scCNN_bank, sc_bank_train_loader, loss, optimizer, 30)\n",
    "\n",
    "sc_bank_predictions = predict(scCNN_bank, sc_bank_test_loader)\n",
    "\n",
    "f1_macro_score = f1_score(sc_bank_data_test[1], sc_bank_predictions, average = 'macro')\n",
    "f1_micro_score = f1_score(sc_bank_data_test[1], sc_bank_predictions, average = 'micro')\n",
    "\n",
    "print(\"F1-measure with macro averaging = \", f1_macro_score)\n",
    "print(\"F1-measure with micro averaging = \", f1_micro_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For telecom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scCNN_telecom = SentClassCNN()\n",
    "\n",
    "scCNN_telecom.type(torch.cuda.FloatTensor)\n",
    "scCNN_telecom.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = optim.Adam(scCNN_telecom.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-measure with macro averaging =  0.7373973696006297\n",
      "F1-measure with micro averaging =  0.8667776852622814\n",
      "Wall time: 44.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_model(scCNN_telecom, sc_telecom_train_loader, loss, optimizer, 30)\n",
    "\n",
    "sc_telecom_predictions = predict(scCNN_telecom, sc_telecom_test_loader)\n",
    "\n",
    "f1_macro_score = f1_score(sc_telecom_data_test[1], sc_telecom_predictions, average = 'macro')\n",
    "f1_micro_score = f1_score(sc_telecom_data_test[1], sc_telecom_predictions, average = 'micro')\n",
    "\n",
    "print(\"F1-measure with macro averaging = \", f1_macro_score)\n",
    "print(\"F1-measure with micro averaging = \", f1_micro_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:\n",
    "### Using our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to build our FastText model\n",
    "\n",
    "We split the csv in 6 parts and we start to train a model with one part than we train it with the others part and save it on the disk. This avoid memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def create_model():\n",
    "    model = FastText(size = 300)\n",
    "    for i in range(0, 6):\n",
    "        print(i)\n",
    "        tweets = pd.read_csv('tweets/tweets_%d.csv' %i, names = ['tweets'])\n",
    "        tweets = tweets.values.reshape(len(tweets)).tolist()\n",
    "        tweets = list(map(str, tweets))\n",
    "        tweets = list(map(lambda t : t.split(), tweets))\n",
    "        \n",
    "        if (i != 0):\n",
    "            model.build_vocab(sentences = tweets, update = True)\n",
    "        else:\n",
    "            model.build_vocab(sentences = tweets)\n",
    "\n",
    "        model.train(sentences = tweets, total_examples = len(tweets), epochs = 5)\n",
    "    \n",
    "    model.save('model/tweets_model')\n",
    "    del tweets\n",
    "    del model\n",
    "    \n",
    "create_model()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_model = FastText.load('model/tweets_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc2_bank_data_train = deepcopy(bank_data_train)\n",
    "sc2_bank_data_test = deepcopy(bank_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc2_bank_data_train[0] = convert_to_vectors(tweets_model, sc2_bank_data_train[0])\n",
    "sc2_bank_data_test[0] = convert_to_vectors(tweets_model, sc2_bank_data_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating bank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc2_bank_dataset_train = SC_Dataset(sc2_bank_data_train, tweets_model.vector_size)\n",
    "sc2_bank_dataset_test = SC_Dataset(sc2_bank_data_test, tweets_model.vector_size)\n",
    "\n",
    "sc2_bank_train_loader = torch.utils.data.DataLoader(sc2_bank_dataset_train, batch_size = batch_size)\n",
    "sc2_bank_test_loader = torch.utils.data.DataLoader(sc2_bank_dataset_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load telecom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc2_telecom_data_train = deepcopy(telecom_data_train)\n",
    "sc2_telecom_data_test = deepcopy(telecom_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc2_telecom_data_train[0] = convert_to_vectors(tweets_model, sc2_telecom_data_train[0])\n",
    "sc2_telecom_data_test[0] = convert_to_vectors(tweets_model, sc2_telecom_data_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating telecom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc2_telecom_dataset_train = SC_Dataset(sc2_telecom_data_train, tweets_model.vector_size)\n",
    "sc2_telecom_dataset_test = SC_Dataset(sc2_telecom_data_test, tweets_model.vector_size)\n",
    "\n",
    "sc2_telecom_train_loader = torch.utils.data.DataLoader(sc2_telecom_dataset_train, batch_size = batch_size)\n",
    "sc2_telecom_test_loader = torch.utils.data.DataLoader(sc2_telecom_dataset_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc2CNN_bank = SentClassCNN()\n",
    "\n",
    "sc2CNN_bank.type(torch.cuda.FloatTensor)\n",
    "sc2CNN_bank.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = optim.Adam(sc2CNN_bank.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-measure with macro averaging =  0.8079423731961528\n",
      "F1-measure with micro averaging =  0.852858481724461\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_model(sc2CNN_bank, sc2_bank_train_loader, loss, optimizer, 30)\n",
    "\n",
    "sc2_bank_predictions = predict(sc2CNN_bank, sc2_bank_test_loader)\n",
    "\n",
    "f1_macro_score = f1_score(sc2_bank_data_test[1], sc2_bank_predictions, average = 'macro')\n",
    "f1_micro_score = f1_score(sc2_bank_data_test[1], sc2_bank_predictions, average = 'micro')\n",
    "\n",
    "print(\"F1-measure with macro averaging = \", f1_macro_score)\n",
    "print(\"F1-measure with micro averaging = \", f1_micro_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For telecom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc2CNN_telecom = SentClassCNN()\n",
    "\n",
    "sc2CNN_telecom.type(torch.cuda.FloatTensor)\n",
    "sc2CNN_telecom.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = optim.Adam(sc2CNN_telecom.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-measure with macro averaging =  0.7373370658735019\n",
      "F1-measure with micro averaging =  0.8501248959200666\n",
      "Wall time: 44.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_model(sc2CNN_telecom, sc2_telecom_train_loader, loss, optimizer, 30)\n",
    "\n",
    "sc2_telecom_predictions = predict(sc2CNN_telecom, sc2_telecom_test_loader)\n",
    "\n",
    "f1_macro_score = f1_score(sc2_telecom_data_test[1], sc2_telecom_predictions, average = 'macro')\n",
    "f1_micro_score = f1_score(sc2_telecom_data_test[1], sc2_telecom_predictions, average = 'micro')\n",
    "\n",
    "print(\"F1-measure with macro averaging = \", f1_macro_score)\n",
    "print(\"F1-measure with micro averaging = \", f1_micro_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
